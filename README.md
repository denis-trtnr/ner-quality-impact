# NER Quality Impact  

This project investigates how **data quality** influences the performance of different **Named Entity Recognition (NER)** models. It systematically introduces **noise** into datasets (train, validation, and test sets) at varying **noise rates** and **stages** to measure robustness and generalization of various NER architectures.

> ğŸ“š **This work was conducted as part of a study in collaboration with the [DFKI Speech & Language Technology Lab](https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology)**

## ğŸ§© Repository Structure

```bash
ner-quality-impact/
â”‚
â”œâ”€â”€ docs/                     # Research docs
â”œâ”€â”€ notebooks/                # Example notebooks
â”œâ”€â”€ scripts/                  # Execution scripts
â”‚   â”œâ”€â”€ run_profile.sh
â”‚   â””â”€â”€ run_profile_pegasus.sh
â”œâ”€â”€ src/                      # Core source code
â”‚   â”œâ”€â”€ noise/                # Noise generation modules
â”‚   â”‚  â”œâ”€â”€ utils/             # Helper utilities for noise generation
â”‚   â”‚  â”œâ”€â”€ label_noise.py     # Injects noise into labels/entities
â”‚   â”‚  â”œâ”€â”€ orthographic.py    # Orthographic (character-level) noise
â”‚   â”‚  â”œâ”€â”€ registry.py        # Registry for available noise types
â”‚   â”‚  â”œâ”€â”€ semantic.py        # Semantic-level noise (word meaning)
â”‚   â”‚  â””â”€â”€ syntactic.py       # Syntactic noise (structure-based)
â”‚   â”œâ”€â”€ profiles/             # Experiment configurations
â”‚   â”œâ”€â”€ data_preprocessing.py # Data loading and preprocessing
â”‚   â”œâ”€â”€ metrics.py            # Evaluation and scoring metrics
â”‚   â””â”€â”€ train.py              # Training loop and orchestration
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ sweep_config_*.yaml       # W&B sweep configurations
â””â”€â”€ README.md                 # You're looking at it :-)

```

---

## âš™ï¸ Installation

Clone the repository and install the dependencies:

```bash
git clone https://github.com/denis-trtnr/ner-quality-impact.git
```
```bash
cd ner-quality-impact
```
```bash
pip install -r requirements.txt
```

## ğŸš€ Running Experiments

You can run experiments in multiple ways depending on your setup.

### 1ï¸âƒ£ Manual Execution
```bash
python -m src.train \
    --model bert-base-cased \
    --profile src/profiles/<PROFILE> \
    --epochs 5 \
    --batch_size 16 \
    --lr 3e-5 \
    --max_length 256 \
    --seed 42
```
---


### 2ï¸âƒ£ Using Provided Shell Scripts
ğŸ’» Local Execution
```bash
bash scripts/run_profile.sh bert-base-cased src/profiles/orthographic/orthographic_p0.1_test_all.yaml

```
ğŸ¦„ [Pegasus](https://pegasus.dfki.de/) Cluster 
```bash
bash scripts/run_profile_pegasus.sh bert-base-cased src/profiles/orthographic/orthographic_p0.1_test_all.yaml

```
---

### 3ï¸âƒ£ Running W&B Sweeps
This project uses **grid search sweeps** with **Weights & Biases (W&B)** to automate structured experiments.  
Each agent executes **one training run after another**, iterating through all defined configurations in sequence.

Example sweep config files:
- `sweep_config_baseline.yaml`
- `sweep_config_test.yaml`
- `sweep_config_train_validation_test.yaml`
- `sweep_config_train_validation.yaml`

Run a sweep:

```bash
wandb sweep sweep_config_baseline.yaml
wandb agent <YOUR_SWEEP_ID>
```

Example for using on cluster (using [Pegasus Bridle Wrapper](https://github.com/DFKI-NLP/pegasus-bridle)):
```bash
bash /home/dtrautner/dev/pegasus-bridle/wrapper.sh wandb agent <YOUR_SWEEP_ID>
```


